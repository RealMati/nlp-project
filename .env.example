# LLM Configuration
LLM_PROVIDER=gemini  # Options: gemini, openai, anthropic, ollama
LLM_API_KEY=your_gemini_api_key
LLM_MODEL_NAME=gemini-1.5-flash # or gpt-4o, llama3
# LLM_API_BASE=http://localhost:11434/v1 # Uncomment for Ollama/LocalAI
