{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-to-SQL V6-SQL - Direct SQL Output (Kaggle Split)\n",
    "\n",
    "**Dataset:** WikiSQL from Salesforce GitHub\n",
    "- **56k train / 8k validation / 15k test**\n",
    "- Schema (table headers) in every example\n",
    "- Downloads directly — no HuggingFace script issues\n",
    "\n",
    "**Key Difference from V6-Structured:**\n",
    "- **V6-Structured**: Model outputs structured indices (sel, agg, conds) → converted to SQL\n",
    "- **V6-SQL**: Model outputs **executable SQL directly**\n",
    "  - Uses `convert_wikisql.py` logic to pre-generate SQL for training\n",
    "  - End-to-end: NL question → SQL query\n",
    "\n",
    "**Expected:** 45-65% execution accuracy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install\n",
    "!pip install -q transformers>=4.35.0 datasets>=2.14.0 accelerate>=0.24.0\n",
    "!pip install -q torch sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEXT-TO-SQL V6-SQL - DIRECT SQL OUTPUT (KAGGLE)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "MODEL_NAME = \"google-t5/t5-base\" if torch.cuda.is_available() else \"google-t5/t5-small\"\n",
    "print(f\"Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Download WikiSQL directly from source\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "DATA_URL = \"https://github.com/salesforce/WikiSQL/raw/master/data.tar.bz2\"\n",
    "DATA_DIR = \"./wikisql_data\"\n",
    "\n",
    "print(\"Downloading WikiSQL from Salesforce GitHub...\")\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    urllib.request.urlretrieve(DATA_URL, \"data.tar.bz2\")\n",
    "    print(\"Extracting...\")\n",
    "    with tarfile.open(\"data.tar.bz2\", \"r:bz2\") as tar:\n",
    "        tar.extractall(\".\")\n",
    "    os.rename(\"data\", DATA_DIR)\n",
    "    os.remove(\"data.tar.bz2\")\n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    print(\"Already downloaded.\")\n",
    "\n",
    "print(f\"\\nFiles: {os.listdir(DATA_DIR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: SQL Generation Utilities (from convert_wikisql.py)\n",
    "AGG_OPS = [\"\", \"MAX\", \"MIN\", \"COUNT\", \"SUM\", \"AVG\"]\n",
    "OPS = ['=', '>', '<', '>=', '<=', '!=']\n",
    "\n",
    "SQL_RESERVED = {\n",
    "    'order', 'group', 'table', 'index', 'select', 'from', 'where', 'join',\n",
    "    'left', 'right', 'inner', 'outer', 'on', 'as', 'and', 'or', 'not',\n",
    "    'limit', 'offset', 'union', 'all', 'distinct', 'null', 'is', 'like',\n",
    "    'between', 'in', 'exists', 'case', 'when', 'then', 'else', 'end',\n",
    "    'count', 'sum', 'avg', 'min', 'max', 'having', 'by', 'asc', 'desc',\n",
    "    'primary', 'key', 'foreign', 'references', 'constraint', 'unique',\n",
    "    'check', 'default', 'create', 'alter', 'drop', 'insert', 'update', 'delete',\n",
    "    'to', 'with', 'into', 'values', 'set', 'call', 'return', 'returning',\n",
    "    'current', 'timestamp', 'user', 'session', 'system', 'date', 'time',\n",
    "    'datetime', 'year', 'month', 'day', 'hour', 'minute', 'second',\n",
    "}\n",
    "\n",
    "def clean_column_name(col, used_names=None):\n",
    "    \"\"\"Convert column name to valid SQL identifier.\"\"\"\n",
    "    cleaned = re.sub(r'[^a-zA-Z0-9_]', '_', col)\n",
    "    cleaned = re.sub(r'_+', '_', cleaned).strip('_')\n",
    "    if cleaned and cleaned[0].isdigit():\n",
    "        cleaned = 'col_' + cleaned\n",
    "    if not cleaned:\n",
    "        cleaned = 'col'\n",
    "    cleaned = cleaned.lower()\n",
    "    if cleaned in SQL_RESERVED:\n",
    "        cleaned = f'\"{cleaned}\"'\n",
    "    if used_names is not None:\n",
    "        base_name = cleaned\n",
    "        suffix = 0\n",
    "        while cleaned in used_names:\n",
    "            suffix += 1\n",
    "            if base_name.startswith('\"') and base_name.endswith('\"'):\n",
    "                cleaned = f'\"{base_name[1:-1]}_{suffix}\"'\n",
    "            else:\n",
    "                cleaned = f'{base_name}_{suffix}'\n",
    "        used_names.add(cleaned)\n",
    "    return cleaned\n",
    "\n",
    "def get_column_names(headers):\n",
    "    \"\"\"Generate clean column names with duplicate handling.\"\"\"\n",
    "    used_names = set()\n",
    "    return [clean_column_name(h, used_names) for h in headers]\n",
    "\n",
    "def value_to_sql(value):\n",
    "    \"\"\"Convert a value to SQL literal.\"\"\"\n",
    "    if isinstance(value, str):\n",
    "        escaped = value.replace(\"'\", \"''\")\n",
    "        return f\"'{escaped}'\"\n",
    "    elif isinstance(value, bool):\n",
    "        return '1' if value else '0'\n",
    "    elif value is None:\n",
    "        return 'NULL'\n",
    "    else:\n",
    "        return str(value)\n",
    "\n",
    "def build_where_clause(conds, col_map):\n",
    "    \"\"\"Build WHERE clause from conditions.\"\"\"\n",
    "    if not conds:\n",
    "        return ''\n",
    "    clauses = []\n",
    "    for col_idx, op, val in conds:\n",
    "        if col_idx in col_map:\n",
    "            col_name = col_map[col_idx]\n",
    "            val_sql = value_to_sql(val)\n",
    "            op_str = OPS[op] if op < len(OPS) else '='\n",
    "            clauses.append(f\"{col_name} {op_str} {val_sql}\")\n",
    "    return ' WHERE ' + ' AND '.join(clauses) if clauses else ''\n",
    "\n",
    "def build_select_clause(sql_info, col_map, table_name):\n",
    "    \"\"\"Build SELECT clause.\"\"\"\n",
    "    sel = sql_info.get('sel', 0)\n",
    "    agg = sql_info.get('agg', 0)\n",
    "    col_name = col_map.get(sel, '*')\n",
    "    if agg == 0:\n",
    "        return f\"SELECT {col_name} FROM {table_name}\"\n",
    "    else:\n",
    "        agg_name = AGG_OPS[agg] if agg < len(AGG_OPS) else \"\"\n",
    "        return f\"SELECT {agg_name}({col_name}) FROM {table_name}\"\n",
    "\n",
    "def convert_to_sql(query, table, table_name=\"t1\"):\n",
    "    \"\"\"Convert WikiSQL query to executable SQL (from convert_wikisql.py).\"\"\"\n",
    "    headers = table.get('header', [])\n",
    "    sql_info = query.get('sql', {})\n",
    "    \n",
    "    # Create column index to name mapping\n",
    "    col_names = get_column_names(headers)\n",
    "    col_map = {i: col_names[i] for i in range(len(col_names))}\n",
    "    \n",
    "    select = build_select_clause(sql_info, col_map, table_name)\n",
    "    where = build_where_clause(sql_info.get('conds', []), col_map)\n",
    "    \n",
    "    return (select + where).strip()\n",
    "\n",
    "# Test with example from convert_wikisql.py\n",
    "test_query = {\n",
    "    'sql': {'sel': 5, 'agg': 0, 'conds': [[3, 0, 'SOUTH AUSTRALIA']]}\n",
    "}\n",
    "test_table = {\n",
    "    'header': ['State/territory', 'Text/background colour', 'Format', 'Current slogan', 'Current series', 'Notes'],\n",
    "    'id': '1-1000181-1'\n",
    "}\n",
    "test_sql = convert_to_sql(test_query, test_table, \"t1\")\n",
    "print(f\"Test SQL: {test_sql}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Load and process WikiSQL - DIRECT SQL OUTPUT\n",
    "from datasets import Dataset\n",
    "\n",
    "def load_wikisql_sql_split(split_name):\n",
    "    \"\"\"Load WikiSQL with executable SQL as target (not structured indices).\"\"\"\n",
    "    main_file = f\"{DATA_DIR}/{split_name}.jsonl\"\n",
    "    tables_file = f\"{DATA_DIR}/{split_name}.tables.jsonl\"\n",
    "    \n",
    "    # Load tables\n",
    "    with open(tables_file) as f:\n",
    "        tables = {t[\"id\"]: t for t in (json.loads(l) for l in f)}\n",
    "    \n",
    "    examples = []\n",
    "    with open(main_file) as f:\n",
    "        for line in f:\n",
    "            row = json.loads(line)\n",
    "            table = tables[row[\"table_id\"]]\n",
    "            \n",
    "            question = row[\"question\"]\n",
    "            header = table[\"header\"]\n",
    "            table_name = table.get(\"name\", \"t1\")\n",
    "            \n",
    "            # Schema string\n",
    "            schema = f\"{table_name}({', '.join(header)})\"\n",
    "            \n",
    "            # Input: question + schema\n",
    "            input_text = f\"translate to SQL: {question} | schema: {schema}\"\n",
    "            \n",
    "            # Output: EXECUTABLE SQL (using convert_wikisql.py logic)\n",
    "            target_sql = convert_to_sql(row, table, table_name)\n",
    "            \n",
    "            examples.append({\n",
    "                \"input_text\": input_text,\n",
    "                \"target_text\": target_sql\n",
    "            })\n",
    "    \n",
    "    return examples\n",
    "\n",
    "print(\"Loading WikiSQL with SQL targets...\")\n",
    "train_data = load_wikisql_sql_split(\"train\")\n",
    "val_data = load_wikisql_sql_split(\"dev\")\n",
    "test_data = load_wikisql_sql_split(\"test\")\n",
    "\n",
    "print(f\"Train: {len(train_data)} | Val: {len(val_data)} | Test: {len(test_data)}\")\n",
    "\n",
    "# Convert to HuggingFace datasets\n",
    "train_ds = Dataset.from_list(train_data)\n",
    "val_ds = Dataset.from_list(val_data)\n",
    "\n",
    "print(f\"\\nSample (SQL output):\")\n",
    "print(f\"Input: {train_ds[0]['input_text'][:100]}...\")\n",
    "print(f\"Target SQL: {train_ds[0]['target_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Tokenize\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize(batch):\n",
    "    inputs = tokenizer(batch[\"input_text\"], max_length=256, truncation=True)\n",
    "    targets = tokenizer(text_target=batch[\"target_text\"], max_length=128, truncation=True)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "print(\"Tokenizing...\")\n",
    "train_tok = train_ds.map(tokenize, batched=True, remove_columns=train_ds.column_names)\n",
    "val_tok = val_ds.map(tokenize, batched=True, remove_columns=val_ds.column_names)\n",
    "\n",
    "print(f\"Train: {len(train_tok)} | Val: {len(val_tok)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Model\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "model.gradient_checkpointing_enable()\n",
    "print(f\"Loaded {MODEL_NAME} ({model.num_parameters():,} params)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Training Config\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=-100)\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./t2sql_v6_sql\",\n",
    "    \n",
    "    num_train_epochs=5,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_ratio=0.05,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=1.0,\n",
    "    \n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=2,\n",
    "    \n",
    "    fp16=torch.cuda.is_available(),\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    generation_num_beams=4,\n",
    "    \n",
    "    logging_steps=100,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=2,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"Epochs: {args.num_train_epochs} | LR: {args.learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Metrics - SQL Execution Accuracy\n",
    "import sqlite3\n",
    "\n",
    "def normalize_sql(sql):\n",
    "    \"\"\"Normalize SQL for comparison.\"\"\"\n",
    "    return ' '.join(sql.lower().strip().split())\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"Compute exact match on generated SQL.\"\"\"\n",
    "    preds, labels = pred\n",
    "    preds = np.clip(preds, 0, len(tokenizer)-1)\n",
    "    pred_str = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    labels = np.clip(labels, 0, len(tokenizer)-1)\n",
    "    label_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Exact match on normalized SQL\n",
    "    exact = sum(normalize_sql(p) == normalize_sql(l) for p, l in zip(pred_str, label_str))\n",
    "    return {\"exact_match\": exact / len(pred_str)}\n",
    "\n",
    "print(\"Metrics: SQL exact match (normalized)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\"Trainer ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Quick sanity check\n",
    "batch = collator([train_tok[i] for i in range(4)])\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    loss = model(**{k: v.to(model.device) for k, v in batch.items()}).loss.item()\n",
    "print(f\"Initial loss: {loss:.2f}\")\n",
    "if loss < 10:\n",
    "    print(\"✓ Ready to train\")\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: TRAIN\n",
    "print(\"=\" * 50)\n",
    "print(\"TRAINING V6-SQL - DIRECT SQL OUTPUT\")\n",
    "print(f\"{len(train_tok)} examples | 5 epochs\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "result = trainer.train()\n",
    "\n",
    "print(f\"\\nDone! Loss: {result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Evaluate\n",
    "ev = trainer.evaluate()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"V6-SQL DIRECT SQL OUTPUT RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Eval Loss: {ev['eval_loss']:.4f}\")\n",
    "print(f\"Exact Match: {ev['eval_exact_match']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Inference Demo - Direct SQL Generation\n",
    "from transformers import pipeline\n",
    "\n",
    "trainer.save_model(\"./t2sql_v6_sql_final\")\n",
    "tokenizer.save_pretrained(\"./t2sql_v6_sql_final\")\n",
    "\n",
    "gen = pipeline(\"text2text-generation\", model=\"./t2sql_v6_sql_final\",\n",
    "               device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "def text_to_sql(question, schema):\n",
    "    \"\"\"Convert question directly to executable SQL.\"\"\"\n",
    "    inp = f\"translate to SQL: {question} | schema: {schema}\"\n",
    "    result = gen(inp, max_length=128, num_beams=4)[0]['generated_text']\n",
    "    return result\n",
    "\n",
    "# Test examples\n",
    "tests = [\n",
    "    (\"How many players are there?\", \"players(id, name, age, team)\"),\n",
    "    (\"What is the total population?\", \"countries(name, population, area)\"),\n",
    "    (\"Show all products under $50\", \"products(id, name, price, category)\"),\n",
    "    (\"What is the average age?\", \"employees(id, name, age, department)\"),\n",
    "]\n",
    "\n",
    "print(\"\\nDirect SQL predictions:\")\n",
    "print(\"-\" * 50)\n",
    "for q, schema in tests:\n",
    "    sql = text_to_sql(q, schema)\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"SQL: {sql}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Execute Generated SQL (Optional Demo)\n",
    "def execute_sql(sql, schema_data):\n",
    "    \"\"\"Execute generated SQL against in-memory SQLite.\"\"\"\n",
    "    conn = sqlite3.connect(':memory:')\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    # Create table and insert data\n",
    "    # ... (implementation depends on your data)\n",
    "    \n",
    "    try:\n",
    "        cursor.execute(sql)\n",
    "        return cursor.fetchall()\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Example: Execute a generated query\n",
    "demo_sql = text_to_sql(\"Count all employees\", \"employees(id, name, dept)\")\n",
    "print(f\"Generated: {demo_sql}\")\n",
    "# result = execute_sql(demo_sql, employees_data)\n",
    "# print(f\"Result: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Save & Report\n",
    "import shutil\n",
    "\n",
    "report = {\n",
    "    \"version\": \"v6_sql_direct_output\",\n",
    "    \"dataset\": \"WikiSQL (direct download)\",\n",
    "    \"output_type\": \"executable_sql\",\n",
    "    \"train_examples\": len(train_tok),\n",
    "    \"train_loss\": result.training_loss,\n",
    "    \"eval_loss\": ev['eval_loss'],\n",
    "    \"exact_match\": ev['eval_exact_match']*100,\n",
    "}\n",
    "\n",
    "json.dump(report, open(\"report_v6_sql.json\", \"w\"), indent=2)\n",
    "shutil.make_archive(\"t2sql_v6_sql\", \"zip\", \".\", \"t2sql_v6_sql_final\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SAVED\")\n",
    "print(\"=\" * 50)\n",
    "print(json.dumps(report, indent=2))\n",
    "print(\"\\nDownload: t2sql_v6_sql.zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
