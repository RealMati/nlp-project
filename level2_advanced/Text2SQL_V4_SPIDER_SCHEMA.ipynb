{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-to-SQL V4 - Spider with REAL Schema\n",
    "\n",
    "**SETUP REQUIRED:**\n",
    "1. Click **+ Add Input** (right sidebar)\n",
    "2. Search: `yale-universitys-spider-10-nlp-dataset`\n",
    "3. Add the dataset by `jeromeblanchet`\n",
    "\n",
    "**What's Fixed:**\n",
    "- Uses `tables.json` for real schema (table names, column names, types)\n",
    "- Merges schema into each training example by `db_id`\n",
    "- Model finally sees actual columns like `students(id, name, gpa)`\n",
    "\n",
    "**Expected:** 35-50% accuracy (vs 11% without schema)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install\n",
    "!pip install -q transformers>=4.35.0 datasets>=2.14.0 accelerate>=0.24.0\n",
    "!pip install -q torch sentencepiece pandas numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup & Verify Kaggle Dataset\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEXT-TO-SQL V4 - SPIDER WITH SCHEMA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Find the Kaggle input path\n",
    "KAGGLE_INPUT = \"/kaggle/input\"\n",
    "SPIDER_PATH = None\n",
    "\n",
    "if os.path.exists(KAGGLE_INPUT):\n",
    "    for folder in os.listdir(KAGGLE_INPUT):\n",
    "        if \"spider\" in folder.lower():\n",
    "            SPIDER_PATH = os.path.join(KAGGLE_INPUT, folder)\n",
    "            break\n",
    "\n",
    "if SPIDER_PATH is None:\n",
    "    print(\"ERROR: Spider dataset not found!\")\n",
    "    print(\"Please add dataset: jeromeblanchet/yale-universitys-spider-10-nlp-dataset\")\n",
    "    raise FileNotFoundError(\"Add Spider dataset first\")\n",
    "\n",
    "print(f\"Spider dataset found: {SPIDER_PATH}\")\n",
    "print(f\"\\nContents:\")\n",
    "for item in os.listdir(SPIDER_PATH)[:10]:\n",
    "    print(f\"  {item}\")\n",
    "\n",
    "print(f\"\\nPyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    GPU_NAME = torch.cuda.get_device_name(0)\n",
    "    GPU_MEM = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {GPU_NAME} ({GPU_MEM:.1f} GB)\")\n",
    "    MODEL_NAME = \"google-t5/t5-base\"\n",
    "else:\n",
    "    MODEL_NAME = \"google-t5/t5-small\"\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load tables.json (THE KEY FIX)\n",
    "import glob\n",
    "\n",
    "# Find tables.json\n",
    "tables_files = glob.glob(f\"{SPIDER_PATH}/**/tables.json\", recursive=True)\n",
    "if not tables_files:\n",
    "    # Try without recursion\n",
    "    tables_files = [f\"{SPIDER_PATH}/tables.json\"]\n",
    "\n",
    "TABLES_JSON = tables_files[0] if tables_files else None\n",
    "\n",
    "if TABLES_JSON and os.path.exists(TABLES_JSON):\n",
    "    print(f\"Found: {TABLES_JSON}\")\n",
    "    with open(TABLES_JSON) as f:\n",
    "        tables_data = json.load(f)\n",
    "    print(f\"Loaded {len(tables_data)} database schemas\")\n",
    "else:\n",
    "    print(\"ERROR: tables.json not found!\")\n",
    "    print(f\"Searched in: {SPIDER_PATH}\")\n",
    "    raise FileNotFoundError(\"tables.json missing\")\n",
    "\n",
    "# Create lookup: db_id -> schema\n",
    "SCHEMA_LOOKUP = {db[\"db_id\"]: db for db in tables_data}\n",
    "\n",
    "# Show example\n",
    "example_db = list(SCHEMA_LOOKUP.keys())[0]\n",
    "example_schema = SCHEMA_LOOKUP[example_db]\n",
    "print(f\"\\nExample schema for '{example_db}':\")\n",
    "print(f\"  Tables: {example_schema.get('table_names', [])[:5]}\")\n",
    "print(f\"  Columns: {example_schema.get('column_names', [])[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load Train/Dev Data\n",
    "\n",
    "# Find train and dev json files\n",
    "train_files = glob.glob(f\"{SPIDER_PATH}/**/train*.json\", recursive=True)\n",
    "dev_files = glob.glob(f\"{SPIDER_PATH}/**/dev*.json\", recursive=True)\n",
    "\n",
    "# Filter out tables.json\n",
    "train_files = [f for f in train_files if \"tables\" not in f.lower()]\n",
    "dev_files = [f for f in dev_files if \"tables\" not in f.lower()]\n",
    "\n",
    "print(f\"Train files: {train_files}\")\n",
    "print(f\"Dev files: {dev_files}\")\n",
    "\n",
    "# Load data\n",
    "with open(train_files[0]) as f:\n",
    "    train_data = json.load(f)\n",
    "with open(dev_files[0]) as f:\n",
    "    dev_data = json.load(f)\n",
    "\n",
    "print(f\"\\nLoaded: {len(train_data)} train, {len(dev_data)} dev examples\")\n",
    "\n",
    "# Show example\n",
    "print(f\"\\nExample train item keys: {list(train_data[0].keys())}\")\n",
    "print(f\"Question: {train_data[0].get('question', '')}\")\n",
    "print(f\"Query: {train_data[0].get('query', '')}\")\n",
    "print(f\"DB ID: {train_data[0].get('db_id', '')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Schema Serialization Function\n",
    "\n",
    "def serialize_schema(db_id):\n",
    "    \"\"\"\n",
    "    Convert schema to string format: table1(col1, col2) | table2(col3, col4)\n",
    "    \"\"\"\n",
    "    if db_id not in SCHEMA_LOOKUP:\n",
    "        return f\"database: {db_id}\"\n",
    "    \n",
    "    schema = SCHEMA_LOOKUP[db_id]\n",
    "    table_names = schema.get(\"table_names\", [])\n",
    "    column_names = schema.get(\"column_names\", [])  # [[table_idx, col_name], ...]\n",
    "    \n",
    "    # Group columns by table\n",
    "    table_cols = defaultdict(list)\n",
    "    \n",
    "    for col_info in column_names:\n",
    "        if not isinstance(col_info, list) or len(col_info) < 2:\n",
    "            continue\n",
    "        table_idx, col_name = col_info[0], col_info[1]\n",
    "        \n",
    "        # Skip * column (table_idx = -1)\n",
    "        if table_idx < 0 or table_idx >= len(table_names):\n",
    "            continue\n",
    "        \n",
    "        table_name = table_names[table_idx].lower().replace(\" \", \"_\")\n",
    "        col_name = col_name.lower().replace(\" \", \"_\")\n",
    "        table_cols[table_name].append(col_name)\n",
    "    \n",
    "    # Build schema string\n",
    "    if table_cols:\n",
    "        parts = [f\"{tbl}({', '.join(cols)})\" for tbl, cols in table_cols.items()]\n",
    "        return \" | \".join(parts)\n",
    "    \n",
    "    return f\"database: {db_id}\"\n",
    "\n",
    "# Test on a few examples\n",
    "print(\"Schema serialization test:\")\n",
    "for i in range(3):\n",
    "    db_id = train_data[i][\"db_id\"]\n",
    "    schema = serialize_schema(db_id)\n",
    "    print(f\"\\n{db_id}:\")\n",
    "    print(f\"  {schema[:100]}...\" if len(schema) > 100 else f\"  {schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: SQL Normalization\n",
    "\n",
    "def normalize_sql(sql):\n",
    "    \"\"\"Normalize SQL for fair comparison.\"\"\"\n",
    "    if not sql:\n",
    "        return \"\"\n",
    "    \n",
    "    sql = str(sql).strip().lower()\n",
    "    sql = ' '.join(sql.split())  # Normalize whitespace\n",
    "    \n",
    "    # Normalize operators\n",
    "    for op in ['>=', '<=', '!=', '<>', '=', '>', '<']:\n",
    "        sql = sql.replace(op, f' {op} ')\n",
    "    \n",
    "    sql = sql.replace(',', ', ')\n",
    "    sql = ' '.join(sql.split())  # Clean up extra spaces\n",
    "    sql = sql.rstrip(';').strip()\n",
    "    \n",
    "    return sql\n",
    "\n",
    "print(\"SQL normalization ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Create HuggingFace Dataset with Schema\n",
    "from datasets import Dataset\n",
    "\n",
    "def process_examples(data_list):\n",
    "    \"\"\"Convert raw data to training format with schema.\"\"\"\n",
    "    processed = []\n",
    "    \n",
    "    for item in data_list:\n",
    "        question = item.get(\"question\", \"\").strip()\n",
    "        query = item.get(\"query\", \"\").strip()\n",
    "        db_id = item.get(\"db_id\", \"\")\n",
    "        \n",
    "        # Get real schema!\n",
    "        schema = serialize_schema(db_id)\n",
    "        \n",
    "        # Format input\n",
    "        input_text = f\"translate to SQL: {question} | schema: {schema}\"\n",
    "        target_text = normalize_sql(query)\n",
    "        \n",
    "        processed.append({\n",
    "            \"input_text\": input_text,\n",
    "            \"target_text\": target_text\n",
    "        })\n",
    "    \n",
    "    return processed\n",
    "\n",
    "print(\"Processing train data...\")\n",
    "train_processed = process_examples(train_data)\n",
    "print(\"Processing dev data...\")\n",
    "dev_processed = process_examples(dev_data)\n",
    "\n",
    "# Convert to HuggingFace datasets\n",
    "train_dataset = Dataset.from_list(train_processed)\n",
    "dev_dataset = Dataset.from_list(dev_processed)\n",
    "\n",
    "print(f\"\\nTrain: {len(train_dataset)} | Dev: {len(dev_dataset)}\")\n",
    "\n",
    "# Verify schema is present\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"VERIFICATION - Schema is now included!\")\n",
    "print(\"=\" * 60)\n",
    "for i in range(2):\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"Input: {train_dataset[i]['input_text'][:150]}...\")\n",
    "    print(f\"Target: {train_dataset[i]['target_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Tokenization\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(f\"Loading tokenizer: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "MAX_INPUT = 512\n",
    "MAX_TARGET = 256\n",
    "\n",
    "def tokenize(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples[\"input_text\"],\n",
    "        max_length=MAX_INPUT,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    targets = tokenizer(\n",
    "        text_target=examples[\"target_text\"],\n",
    "        max_length=MAX_TARGET,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "print(\"Tokenizing...\")\n",
    "train_tokenized = train_dataset.map(tokenize, batched=True, remove_columns=train_dataset.column_names)\n",
    "dev_tokenized = dev_dataset.map(tokenize, batched=True, remove_columns=dev_dataset.column_names)\n",
    "\n",
    "print(f\"Done. Columns: {train_tokenized.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Load Model\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "print(f\"Parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Training Config\n",
    "from transformers import (\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./t2sql_v4\",\n",
    "    \n",
    "    num_train_epochs=25,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.06,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    \n",
    "    weight_decay=0.01,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    gradient_checkpointing=True,\n",
    "    label_smoothing_factor=0.1,\n",
    "    \n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=MAX_TARGET,\n",
    "    generation_num_beams=4,\n",
    "    \n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=2,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"Training config:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  LR: {training_args.learning_rate}\")\n",
    "print(f\"  Effective batch: 32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Metrics\n",
    "VOCAB_SIZE = len(tokenizer)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    predictions = np.clip(predictions, 0, VOCAB_SIZE - 1)\n",
    "    \n",
    "    try:\n",
    "        pred_texts = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    except:\n",
    "        return {\"exact_match\": 0.0, \"normalized_match\": 0.0}\n",
    "    \n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    labels = np.clip(labels, 0, VOCAB_SIZE - 1)\n",
    "    \n",
    "    try:\n",
    "        label_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    except:\n",
    "        return {\"exact_match\": 0.0, \"normalized_match\": 0.0}\n",
    "    \n",
    "    exact = 0\n",
    "    normalized = 0\n",
    "    total = len(pred_texts)\n",
    "    \n",
    "    for pred, label in zip(pred_texts, label_texts):\n",
    "        if pred.strip() == label.strip():\n",
    "            exact += 1\n",
    "        if normalize_sql(pred) == normalize_sql(label):\n",
    "            normalized += 1\n",
    "    \n",
    "    return {\n",
    "        \"exact_match\": exact / total if total > 0 else 0.0,\n",
    "        \"normalized_match\": normalized / total if total > 0 else 0.0\n",
    "    }\n",
    "\n",
    "print(\"Metrics ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Initialize Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=dev_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(f\"Trainer ready.\")\n",
    "print(f\"Train: {len(train_tokenized)} | Eval: {len(dev_tokenized)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Verify Setup\n",
    "print(\"Final verification...\")\n",
    "\n",
    "test_batch = [train_tokenized[i] for i in range(2)]\n",
    "collated = data_collator(test_batch)\n",
    "print(\"✓ Collator OK\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(**{k: v.to(model.device) for k, v in collated.items()})\n",
    "print(f\"✓ Forward OK (loss: {out.loss.item():.2f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"V4 READY - NOW WITH REAL SCHEMA!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: TRAIN\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING V4 TRAINING (WITH SCHEMA)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Epochs: 25 | LR: 2e-4 | Batch: 32\")\n",
    "print(f\"Schema: INCLUDED (from tables.json)\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nYou can close browser. Training continues.\\n\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"V4 TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train loss: {result.training_loss:.4f}\")\n",
    "print(f\"Time: {result.metrics['train_runtime']/3600:.2f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Evaluate\n",
    "print(\"Evaluating...\\n\")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"V4 RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Eval Loss: {eval_results['eval_loss']:.4f}\")\n",
    "print(f\"Exact Match: {eval_results['eval_exact_match']*100:.2f}%\")\n",
    "print(f\"Normalized Match: {eval_results['eval_normalized_match']*100:.2f}%\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "nm = eval_results['eval_normalized_match'] * 100\n",
    "if nm >= 40:\n",
    "    grade = \"EXCELLENT\"\n",
    "elif nm >= 30:\n",
    "    grade = \"GOOD\"\n",
    "elif nm >= 20:\n",
    "    grade = \"ACCEPTABLE\"\n",
    "else:\n",
    "    grade = \"NEEDS WORK\"\n",
    "print(f\"\\nGrade: {grade}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Save Model\n",
    "OUTPUT = \"./t2sql_final_v4\"\n",
    "\n",
    "print(f\"Saving to {OUTPUT}...\")\n",
    "trainer.save_model(OUTPUT)\n",
    "tokenizer.save_pretrained(OUTPUT)\n",
    "\n",
    "report = {\n",
    "    \"version\": \"v4_with_schema\",\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"epochs\": 25,\n",
    "    \"learning_rate\": \"2e-4\",\n",
    "    \"schema_source\": \"tables.json\",\n",
    "    \"train_examples\": len(train_data),\n",
    "    \"train_loss\": result.training_loss,\n",
    "    \"eval_loss\": eval_results['eval_loss'],\n",
    "    \"exact_match_pct\": eval_results['eval_exact_match'] * 100,\n",
    "    \"normalized_match_pct\": eval_results['eval_normalized_match'] * 100,\n",
    "    \"training_hours\": result.metrics['train_runtime'] / 3600\n",
    "}\n",
    "\n",
    "with open(\"report_v4.json\", \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(\"Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Test Predictions\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"Testing V4 model...\\n\")\n",
    "\n",
    "gen = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=OUTPUT,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "def predict(question, schema):\n",
    "    inp = f\"translate to SQL: {question} | schema: {schema}\"\n",
    "    out = gen(inp, max_length=256, num_beams=4)\n",
    "    return out[0]['generated_text']\n",
    "\n",
    "# Test with real schemas from training\n",
    "tests = [\n",
    "    (\"How many singers are there?\", serialize_schema(\"concert_singer\")),\n",
    "    (\"Show all stadium names\", serialize_schema(\"concert_singer\")),\n",
    "    (\"Find pets older than 3 years\", serialize_schema(\"pets_1\")),\n",
    "    (\"Count employees per department\", serialize_schema(\"employee_hire_evaluation\")),\n",
    "]\n",
    "\n",
    "for q, s in tests:\n",
    "    sql = predict(q, s)\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"Schema: {s[:60]}...\" if len(s) > 60 else f\"Schema: {s}\")\n",
    "    print(f\"SQL: {sql}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: Zip Model\n",
    "import shutil\n",
    "\n",
    "print(\"Zipping model...\")\n",
    "shutil.make_archive(\"t2sql_v4_model\", \"zip\", \".\", \"t2sql_final_v4\")\n",
    "print(\"Created: t2sql_v4_model.zip\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"V4 FINAL REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(report, indent=2))\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nDownload: t2sql_v4_model.zip\")\n",
    "print(\"\\nThis version includes REAL SCHEMA from tables.json!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
