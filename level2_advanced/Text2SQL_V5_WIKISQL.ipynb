{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Text-to-SQL V5 - WikiSQL (Direct Download)\n\n**Dataset:** WikiSQL from Salesforce GitHub\n- **56k train / 8k validation / 15k test**\n- Schema (table headers) in every example\n- Downloads directly — no HuggingFace script issues\n\n**Expected:** 50-70% accuracy\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install\n",
    "!pip install -q transformers>=4.35.0 datasets>=2.14.0 accelerate>=0.24.0\n",
    "!pip install -q torch sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEXT-TO-SQL V5 - WIKISQL\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "MODEL_NAME = \"google-t5/t5-base\" if torch.cuda.is_available() else \"google-t5/t5-small\"\n",
    "print(f\"Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 3: Download WikiSQL directly from source\nimport urllib.request\nimport tarfile\nimport json\nimport os\n\nDATA_URL = \"https://github.com/salesforce/WikiSQL/raw/master/data.tar.bz2\"\nDATA_DIR = \"./wikisql_data\"\n\nprint(\"Downloading WikiSQL from Salesforce GitHub...\")\nif not os.path.exists(DATA_DIR):\n    # Download\n    urllib.request.urlretrieve(DATA_URL, \"data.tar.bz2\")\n    print(\"Extracting...\")\n    with tarfile.open(\"data.tar.bz2\", \"r:bz2\") as tar:\n        tar.extractall(\".\")\n    os.rename(\"data\", DATA_DIR)\n    os.remove(\"data.tar.bz2\")\n    print(\"Done!\")\nelse:\n    print(\"Already downloaded.\")\n\nprint(f\"\\nFiles: {os.listdir(DATA_DIR)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Load and process WikiSQL\nfrom datasets import Dataset\n\nAGG_OPS = [\"\", \"MAX\", \"MIN\", \"COUNT\", \"SUM\", \"AVG\"]\nCOND_OPS = [\"=\", \">\", \"<\", \"OP\"]\n\ndef make_sql(sel, agg, columns, conds):\n    \"\"\"Convert to human readable SQL\"\"\"\n    sql = f\"SELECT {AGG_OPS[agg]} {columns[sel]} FROM table\"\n    if conds:\n        where = \" AND \".join([f\"{columns[c[0]]} {COND_OPS[c[1]]} {c[2]}\" for c in conds])\n        sql += f\" WHERE {where}\"\n    return \" \".join(sql.split())\n\ndef load_wikisql_split(split_name):\n    \"\"\"Load a WikiSQL split (train/dev/test)\"\"\"\n    main_file = f\"{DATA_DIR}/{split_name}.jsonl\"\n    tables_file = f\"{DATA_DIR}/{split_name}.tables.jsonl\"\n    \n    # Load tables\n    with open(tables_file) as f:\n        tables = {t[\"id\"]: t for t in (json.loads(l) for l in f)}\n    \n    # Load examples\n    examples = []\n    with open(main_file) as f:\n        for line in f:\n            row = json.loads(line)\n            table = tables[row[\"table_id\"]]\n            \n            question = row[\"question\"]\n            header = table[\"header\"]\n            table_name = table.get(\"name\", \"table\")\n            \n            # Build SQL\n            sql = make_sql(\n                row[\"sql\"][\"sel\"],\n                row[\"sql\"][\"agg\"],\n                header,\n                row[\"sql\"][\"conds\"]\n            )\n            \n            # Schema from header\n            schema = f\"{table_name}({', '.join(header)})\"\n            \n            input_text = f\"translate to SQL: {question} | schema: {schema}\"\n            target_text = sql.lower()\n            \n            examples.append({\n                \"input_text\": input_text,\n                \"target_text\": target_text\n            })\n    \n    return examples\n\nprint(\"Loading WikiSQL...\")\ntrain_data = load_wikisql_split(\"train\")\nval_data = load_wikisql_split(\"dev\")\ntest_data = load_wikisql_split(\"test\")\n\nprint(f\"Train: {len(train_data)} | Val: {len(val_data)} | Test: {len(test_data)}\")\n\n# Convert to HuggingFace datasets\ntrain_ds = Dataset.from_list(train_data)\nval_ds = Dataset.from_list(val_data)\n\nprint(f\"\\nSample:\")\nprint(f\"Input: {train_ds[0]['input_text'][:100]}...\")\nprint(f\"Target: {train_ds[0]['target_text']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 5: Tokenize\nfrom transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n\ndef tokenize(batch):\n    inputs = tokenizer(batch[\"input_text\"], max_length=256, truncation=True)\n    targets = tokenizer(text_target=batch[\"target_text\"], max_length=128, truncation=True)\n    inputs[\"labels\"] = targets[\"input_ids\"]\n    return inputs\n\nprint(\"Tokenizing...\")\ntrain_tok = train_ds.map(tokenize, batched=True, remove_columns=train_ds.column_names)\nval_tok = val_ds.map(tokenize, batched=True, remove_columns=val_ds.column_names)\n\nprint(f\"Train: {len(train_tok)} | Val: {len(val_tok)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Model\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "model.gradient_checkpointing_enable()\n",
    "print(f\"Loaded {MODEL_NAME} ({model.num_parameters():,} params)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 7: Training Config\nfrom transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n\ncollator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=-100)\n\n# WikiSQL is bigger, so fewer epochs needed\nargs = Seq2SeqTrainingArguments(\n    output_dir=\"./t2sql_wikisql\",\n    \n    num_train_epochs=5,            # Fewer epochs (more data)\n    learning_rate=1e-4,\n    warmup_ratio=0.05,\n    lr_scheduler_type=\"cosine\",\n    max_grad_norm=1.0,\n    \n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    gradient_accumulation_steps=2,  # Effective batch 32\n    \n    fp16=torch.cuda.is_available(),\n    gradient_checkpointing=True,\n    \n    eval_strategy=\"steps\",\n    eval_steps=1000,\n    save_strategy=\"steps\",\n    save_steps=1000,\n    save_total_limit=2,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"eval_loss\",\n    greater_is_better=False,\n    \n    predict_with_generate=True,\n    generation_max_length=128,\n    generation_num_beams=4,\n    \n    logging_steps=100,\n    report_to=\"none\",\n    dataloader_num_workers=2,\n    seed=42,\n)\n\nprint(f\"Epochs: {args.num_train_epochs} | LR: {args.learning_rate}\")\nprint(f\"Train examples: {len(train_tok)} → ~2-3 hours\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Metrics\n",
    "VOCAB = len(tokenizer)\n",
    "\n",
    "def normalize(sql):\n",
    "    return \" \".join(str(sql).lower().split())\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    preds, labels = pred\n",
    "    preds = np.clip(preds, 0, VOCAB-1)\n",
    "    \n",
    "    pred_str = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    labels = np.clip(labels, 0, VOCAB-1)\n",
    "    label_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    exact = sum(p.strip() == l.strip() for p, l in zip(pred_str, label_str))\n",
    "    norm = sum(normalize(p) == normalize(l) for p, l in zip(pred_str, label_str))\n",
    "    n = len(pred_str)\n",
    "    \n",
    "    return {\"exact_match\": exact/n, \"normalized_match\": norm/n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 9: Trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_tok,\n    eval_dataset=val_tok,\n    tokenizer=tokenizer,\n    data_collator=collator,\n    compute_metrics=compute_metrics,\n)\nprint(\"Trainer ready\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 10: Quick sanity check\nbatch = collator([train_tok[i] for i in range(4)])\nmodel.eval()\nwith torch.no_grad():\n    loss = model(**{k: v.to(model.device) for k, v in batch.items()}).loss.item()\nprint(f\"Initial loss: {loss:.2f}\")\nif loss < 10:\n    print(\"✓ Ready to train\")\nmodel.train()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 11: TRAIN\nprint(\"=\" * 50)\nprint(\"TRAINING V5 WikiSQL\")\nprint(f\"{len(train_tok)} examples | 5 epochs | ~2-3 hours\")\nprint(\"=\" * 50)\n\ntorch.cuda.empty_cache() if torch.cuda.is_available() else None\nresult = trainer.train()\n\nprint(f\"\\nDone! Loss: {result.training_loss:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 12: Evaluate\nev = trainer.evaluate()\n\nprint(\"=\" * 50)\nprint(\"WikiSQL RESULTS\")\nprint(\"=\" * 50)\nprint(f\"Eval Loss: {ev['eval_loss']:.4f}\")\nprint(f\"Exact Match: {ev['eval_exact_match']*100:.1f}%\")\nprint(f\"Normalized Match: {ev['eval_normalized_match']*100:.1f}%\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Test predictions\n",
    "from transformers import pipeline\n",
    "\n",
    "trainer.save_model(\"./t2sql_wikisql_final\")\n",
    "tokenizer.save_pretrained(\"./t2sql_wikisql_final\")\n",
    "\n",
    "gen = pipeline(\"text2text-generation\", model=\"./t2sql_wikisql_final\", \n",
    "               device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "tests = [\n",
    "    (\"How many players are there?\", \"players(id, name, age, team)\"),\n",
    "    (\"What is the total population?\", \"countries(name, population, area)\"),\n",
    "    (\"Show all products under $50\", \"products(id, name, price, category)\"),\n",
    "]\n",
    "\n",
    "print(\"\\nTest predictions:\")\n",
    "for q, schema in tests:\n",
    "    inp = f\"translate to SQL: {q} | schema: {schema}\"\n",
    "    out = gen(inp, max_length=128, num_beams=4)[0]['generated_text']\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"SQL: {out}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 14: Save & Zip\nimport shutil\n\nreport = {\n    \"version\": \"v5_wikisql\",\n    \"dataset\": \"WikiSQL (direct download)\",\n    \"train_examples\": len(train_tok),\n    \"train_loss\": result.training_loss,\n    \"eval_loss\": ev['eval_loss'],\n    \"exact_match\": ev['eval_exact_match']*100,\n    \"normalized_match\": ev['eval_normalized_match']*100,\n}\n\njson.dump(report, open(\"report_v5.json\", \"w\"), indent=2)\nshutil.make_archive(\"t2sql_v5_wikisql\", \"zip\", \".\", \"t2sql_wikisql_final\")\n\nprint(\"=\" * 50)\nprint(\"SAVED\")\nprint(\"=\" * 50)\nprint(json.dumps(report, indent=2))\nprint(\"\\nDownload: t2sql_v5_wikisql.zip\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}