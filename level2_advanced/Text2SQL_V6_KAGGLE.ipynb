{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-to-SQL V6 - Structured Output (Kaggle Split)\n",
    "\n",
    "**Dataset:** WikiSQL from Salesforce GitHub\n",
    "- **56k train / 8k validation / 15k test**\n",
    "- Schema (table headers) in every example\n",
    "- Downloads directly — no HuggingFace script issues\n",
    "\n",
    "**Key Difference from V5:**\n",
    "- **V5**: Model outputs raw SQL text\n",
    "- **V6**: Model outputs **structured indices** (sel, agg, conds)\n",
    "  - At inference, indices are converted to valid SQL\n",
    "  - More generalizable across schemas\n",
    "\n",
    "**Expected:** 50-70% accuracy\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install\n",
    "!pip install -q transformers>=4.35.0 datasets>=2.14.0 accelerate>=0.24.0\n",
    "!pip install -q torch sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEXT-TO-SQL V6 - STRUCTURED OUTPUT (KAGGLE)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "MODEL_NAME = \"google-t5/t5-base\" if torch.cuda.is_available() else \"google-t5/t5-small\"\n",
    "print(f\"Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Download WikiSQL directly from source\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import json\n",
    "import os\n",
    "\n",
    "DATA_URL = \"https://github.com/salesforce/WikiSQL/raw/master/data.tar.bz2\"\n",
    "DATA_DIR = \"./wikisql_data\"\n",
    "\n",
    "print(\"Downloading WikiSQL from Salesforce GitHub...\")\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    # Download\n",
    "    urllib.request.urlretrieve(DATA_URL, \"data.tar.bz2\")\n",
    "    print(\"Extracting...\")\n",
    "    with tarfile.open(\"data.tar.bz2\", \"r:bz2\") as tar:\n",
    "        tar.extractall(\".\")\n",
    "    os.rename(\"data\", DATA_DIR)\n",
    "    os.remove(\"data.tar.bz2\")\n",
    "    print(\"Done!\")\n",
    "else:\n",
    "    print(\"Already downloaded.\")\n",
    "\n",
    "print(f\"\\nFiles: {os.listdir(DATA_DIR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 4: Load and process WikiSQL - STRUCTURED FORMAT\nfrom datasets import Dataset\n\nAGG_OPS = [\"\", \"MAX\", \"MIN\", \"COUNT\", \"SUM\", \"AVG\"]\nOPS = ['=', '>', '<', '>=', '<=', '!=']  # Full operator set (WikiSQL uses 0-2)\n\ndef encode_structured_label(sel, agg, conds):\n    \"\"\"Encode structured output as text for seq2seq training.\"\"\"\n    parts = [f\"SEL:{sel}\", f\"AGG:{agg}\"]\n    if conds:\n        cond_str = \";\".join([f\"{c[0]},{c[1]},{c[2]}\" for c in conds])\n        parts.append(f\"CONDS:{cond_str}\")\n    else:\n        parts.append(\"CONDS:\")\n    return \" | \".join(parts)\n\ndef load_wikisql_split(split_name):\n    \"\"\"Load a WikiSQL split with structured labels.\"\"\"\n    main_file = f\"{DATA_DIR}/{split_name}.jsonl\"\n    tables_file = f\"{DATA_DIR}/{split_name}.tables.jsonl\"\n    \n    # Load tables\n    with open(tables_file) as f:\n        tables = {t[\"id\"]: t for t in (json.loads(l) for l in f)}\n    \n    examples = []\n    with open(main_file) as f:\n        for line in f:\n            row = json.loads(line)\n            table = tables[row[\"table_id\"]]\n            \n            question = row[\"question\"]\n            header = table[\"header\"]\n            table_name = table.get(\"name\", \"table\")\n            \n            # Schema string\n            schema = f\"{table_name}({', '.join(header)})\"\n            \n            # Input: question + schema\n            input_text = f\"translate to SQL: {question} | schema: {schema}\"\n            \n            # Output: structured indices (encoded as string to avoid Arrow type issues)\n            target_text = encode_structured_label(\n                row[\"sql\"][\"sel\"],\n                row[\"sql\"][\"agg\"],\n                row[\"sql\"][\"conds\"]\n            )\n            \n            # Only include string fields for Arrow compatibility\n            examples.append({\n                \"input_text\": input_text,\n                \"target_text\": target_text\n            })\n    \n    return examples\n\nprint(\"Loading WikiSQL...\")\ntrain_data = load_wikisql_split(\"train\")\nval_data = load_wikisql_split(\"dev\")\ntest_data = load_wikisql_split(\"test\")\n\nprint(f\"Train: {len(train_data)} | Val: {len(val_data)} | Test: {len(test_data)}\")\n\n# Convert to HuggingFace datasets\ntrain_ds = Dataset.from_list(train_data)\nval_ds = Dataset.from_list(val_data)\n\nprint(f\"\\nSample structured output:\")\nprint(f\"Input: {train_ds[0]['input_text'][:100]}...\")\nprint(f\"Target: {train_ds[0]['target_text']}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Tokenize\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize(batch):\n",
    "    inputs = tokenizer(batch[\"input_text\"], max_length=256, truncation=True)\n",
    "    targets = tokenizer(text_target=batch[\"target_text\"], max_length=128, truncation=True)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "print(\"Tokenizing...\")\n",
    "train_tok = train_ds.map(tokenize, batched=True, remove_columns=train_ds.column_names)\n",
    "val_tok = val_ds.map(tokenize, batched=True, remove_columns=val_ds.column_names)\n",
    "\n",
    "print(f\"Train: {len(train_tok)} | Val: {len(val_tok)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Model\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "model.gradient_checkpointing_enable()\n",
    "print(f\"Loaded {MODEL_NAME} ({model.num_parameters():,} params)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Training Config\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=-100)\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./t2sql_v6_structured\",\n",
    "    \n",
    "    num_train_epochs=5,\n",
    "    learning_rate=1e-4,\n",
    "    warmup_ratio=0.05,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=1.0,\n",
    "    \n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    gradient_accumulation_steps=2,\n",
    "    \n",
    "    fp16=torch.cuda.is_available(),\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    generation_num_beams=4,\n",
    "    \n",
    "    logging_steps=100,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=2,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"Epochs: {args.num_train_epochs} | LR: {args.learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Cell 8: Structured Decoding & Metrics (using convert_wikisql.py approach)\nimport re\n\n# Full operator set (WikiSQL only uses 0-2, but we support all)\nOPS = ['=', '>', '<', '>=', '<=', '!=']\n\n# SQL reserved words that need quoting\nSQL_RESERVED = {\n    'order', 'group', 'table', 'index', 'select', 'from', 'where', 'join',\n    'left', 'right', 'inner', 'outer', 'on', 'as', 'and', 'or', 'not',\n    'limit', 'offset', 'union', 'all', 'distinct', 'null', 'is', 'like',\n    'between', 'in', 'exists', 'case', 'when', 'then', 'else', 'end',\n    'count', 'sum', 'avg', 'min', 'max', 'having', 'by', 'asc', 'desc',\n    'primary', 'key', 'foreign', 'references', 'constraint', 'unique',\n    'check', 'default', 'create', 'alter', 'drop', 'insert', 'update', 'delete',\n    'to', 'with', 'into', 'values', 'set', 'call', 'return', 'returning',\n    'out', 'inout', 'procedure', 'function', 'trigger', 'view', 'schema',\n    'current', 'timestamp', 'user', 'session', 'system', 'date', 'time',\n    'datetime', 'year', 'month', 'day', 'hour', 'minute', 'second',\n}\n\ndef clean_column_name(col, used_names=None):\n    \"\"\"Convert column name to valid SQL identifier (from convert_wikisql.py).\"\"\"\n    cleaned = re.sub(r'[^a-zA-Z0-9_]', '_', col)\n    cleaned = re.sub(r'_+', '_', cleaned).strip('_')\n    if cleaned and cleaned[0].isdigit():\n        cleaned = 'col_' + cleaned\n    if not cleaned:\n        cleaned = 'col'\n    cleaned = cleaned.lower()\n    if cleaned in SQL_RESERVED:\n        cleaned = f'\"{cleaned}\"'\n    if used_names is not None:\n        base_name = cleaned\n        suffix = 0\n        while cleaned in used_names:\n            suffix += 1\n            if base_name.startswith('\"') and base_name.endswith('\"'):\n                cleaned = f'\"{base_name[1:-1]}_{suffix}\"'\n            else:\n                cleaned = f'{base_name}_{suffix}'\n        used_names.add(cleaned)\n    return cleaned\n\ndef get_column_names(headers):\n    \"\"\"Generate clean column names with duplicate handling.\"\"\"\n    used_names = set()\n    col_names = [clean_column_name(h, used_names) for h in headers]\n    return col_names\n\ndef value_to_sql(value):\n    \"\"\"Convert a value to SQL literal (from convert_wikisql.py).\"\"\"\n    if isinstance(value, str):\n        escaped = value.replace(\"'\", \"''\")\n        return f\"'{escaped}'\"\n    elif value is None:\n        return 'NULL'\n    else:\n        return str(value)\n\ndef decode_structured_output(text):\n    \"\"\"Decode model output into structured components.\"\"\"\n    sel = agg = None\n    conds = []\n    try:\n        for part in text.split(\" | \"):\n            if part.startswith(\"SEL:\"):\n                sel = int(part[4:].strip())\n            elif part.startswith(\"AGG:\"):\n                agg = int(part[4:].strip())\n            elif part.startswith(\"CONDS:\"):\n                cond_str = part[6:].strip()\n                if cond_str:\n                    for c in cond_str.split(\";\"):\n                        vals = c.split(\",\")\n                        if len(vals) >= 3:\n                            conds.append([int(vals[0]), int(vals[1]), vals[2]])\n    except:\n        pass\n    return sel, agg, conds\n\ndef structured_to_sql(sel, agg, conds, header, table_name=\"table\"):\n    \"\"\"Convert structured indices to SQL string (convert_wikisql.py approach).\"\"\"\n    if sel is None or agg is None:\n        return \"\"\n\n    # Get clean column names\n    col_names = get_column_names(header)\n    col_map = {i: col_names[i] for i in range(len(col_names))}\n\n    # SELECT clause\n    col_name = col_map.get(sel, 'col')\n    if agg == 0:\n        sql = f\"SELECT {col_name} FROM {table_name}\"\n    else:\n        agg_op = AGG_OPS[agg] if agg < len(AGG_OPS) else \"\"\n        sql = f\"SELECT {agg_op}({col_name}) FROM {table_name}\"\n\n    # WHERE clause\n    if conds:\n        where_parts = []\n        for c_idx, c_op, c_val in conds:\n            if c_idx in col_map:\n                col_name = col_map[c_idx]\n                val_sql = value_to_sql(c_val)\n                op_str = OPS[c_op] if c_op < len(OPS) else '='\n                where_parts.append(f\"{col_name} {op_str} {val_sql}\")\n        if where_parts:\n            sql += \" WHERE \" + \" AND \".join(where_parts)\n\n    return sql.lower()\n\ndef compute_metrics(pred):\n    \"\"\"Compute exact match on structured components.\"\"\"\n    preds, labels = pred\n    preds = np.clip(preds, 0, len(tokenizer)-1)\n    pred_str = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    labels = np.clip(labels, 0, len(tokenizer)-1)\n    label_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    exact = sum(p.strip() == l.strip() for p, l in zip(pred_str, label_str))\n    return {\"exact_match\": exact / len(pred_str)}\n\n# Test decoding with actual WikiSQL example\ntest_out = \"SEL:5 | AGG:0 | CONDS:3,0,SOUTH AUSTRALIA\"\nsel, agg, conds = decode_structured_output(test_out)\nprint(f\"Decoded: sel={sel}, agg={agg}, conds={conds}\")\n\n# Actual WikiSQL header for table 1-1000181-1\ntest_header = ['State/territory', 'Text/background colour', 'Format', 'Current slogan', 'Current series', 'Notes']\nsql = structured_to_sql(sel, agg, conds, test_header, \"t1\")\nprint(f\"SQL: {sql}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=val_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\"Trainer ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Quick sanity check\n",
    "batch = collator([train_tok[i] for i in range(4)])\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    loss = model(**{k: v.to(model.device) for k, v in batch.items()}).loss.item()\n",
    "print(f\"Initial loss: {loss:.2f}\")\n",
    "if loss < 10:\n",
    "    print(\"✓ Ready to train\")\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: TRAIN\n",
    "print(\"=\" * 50)\n",
    "print(\"TRAINING V6 - STRUCTURED OUTPUT\")\n",
    "print(f\"{len(train_tok)} examples | 5 epochs\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "result = trainer.train()\n",
    "\n",
    "print(f\"\\nDone! Loss: {result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Evaluate\n",
    "ev = trainer.evaluate()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"V6 STRUCTURED OUTPUT RESULTS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Eval Loss: {ev['eval_loss']:.4f}\")\n",
    "print(f\"Exact Match: {ev['eval_exact_match']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Inference Demo\n",
    "from transformers import pipeline\n",
    "\n",
    "trainer.save_model(\"./t2sql_v6_structured_final\")\n",
    "tokenizer.save_pretrained(\"./t2sql_v6_structured_final\")\n",
    "\n",
    "gen = pipeline(\"text2text-generation\", model=\"./t2sql_v6_structured_final\",\n",
    "               device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "def text_to_sql_structured(question, schema):\n",
    "    \"\"\"Convert question to SQL via structured prediction.\"\"\"\n",
    "    inp = f\"translate to SQL: {question} | schema: {schema}\"\n",
    "    out = gen(inp, max_length=128, num_beams=4)[0]['generated_text']\n",
    "    \n",
    "    # Parse schema for headers\n",
    "    table_start = schema.index('(')\n",
    "    table_name = schema[:table_start].strip()\n",
    "    headers = [h.strip() for h in schema[table_start+1:-1].split(',')]\n",
    "    \n",
    "    # Decode structured output\n",
    "    sel, agg, conds = decode_structured_output(out)\n",
    "    sql = structured_to_sql(sel, agg, conds, headers, table_name)\n",
    "    \n",
    "    return sql\n",
    "\n",
    "tests = [\n",
    "    (\"How many players are there?\", \"players(id, name, age, team)\"),\n",
    "    (\"What is the total population?\", \"countries(name, population, area)\"),\n",
    "    (\"Show all products under $50\", \"products(id, name, price, category)\"),\n",
    "]\n",
    "\n",
    "print(\"\\nTest predictions (structured → SQL):\")\n",
    "for q, schema in tests:\n",
    "    sql = text_to_sql_structured(q, schema)\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"SQL: {sql}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Save & Report\n",
    "import shutil\n",
    "\n",
    "report = {\n",
    "    \"version\": \"v6_structured_output\",\n",
    "    \"dataset\": \"WikiSQL (direct download)\",\n",
    "    \"train_examples\": len(train_tok),\n",
    "    \"train_loss\": result.training_loss,\n",
    "    \"eval_loss\": ev['eval_loss'],\n",
    "    \"exact_match\": ev['eval_exact_match']*100,\n",
    "}\n",
    "\n",
    "json.dump(report, open(\"report_v6.json\", \"w\"), indent=2)\n",
    "shutil.make_archive(\"t2sql_v6_structured\", \"zip\", \".\", \"t2sql_v6_structured_final\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"SAVED\")\n",
    "print(\"=\" * 50)\n",
    "print(json.dumps(report, indent=2))\n",
    "print(\"\\nDownload: t2sql_v6_structured.zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}