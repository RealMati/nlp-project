{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-to-SQL V5 Spider - FAST & FIXED\n",
    "\n",
    "**V4 crashed because:** Schema serialization produced bad data (loss=112)\n",
    "\n",
    "**V5 Fixes:**\n",
    "- Debug cell to inspect schema BEFORE training\n",
    "- Validate all data (no empty strings)\n",
    "- Keep fast LR (1e-4)\n",
    "- FP16 OFF (stability)\n",
    "- Compact schema format\n",
    "\n",
    "**SETUP:** Add dataset `jeromeblanchet/yale-universitys-spider-10-nlp-dataset`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install\n",
    "!pip install -q transformers>=4.35.0 datasets>=2.14.0 accelerate>=0.24.0\n",
    "!pip install -q torch sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Setup\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import glob\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEXT-TO-SQL V5 - SPIDER FAST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "KAGGLE_INPUT = \"/kaggle/input\"\n",
    "SPIDER_PATH = None\n",
    "\n",
    "for folder in os.listdir(KAGGLE_INPUT):\n",
    "    if \"spider\" in folder.lower():\n",
    "        SPIDER_PATH = os.path.join(KAGGLE_INPUT, folder)\n",
    "        break\n",
    "\n",
    "if not SPIDER_PATH:\n",
    "    raise FileNotFoundError(\"Add: jeromeblanchet/yale-universitys-spider-10-nlp-dataset\")\n",
    "\n",
    "print(f\"Spider: {SPIDER_PATH}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "MODEL_NAME = \"google-t5/t5-base\" if torch.cuda.is_available() else \"google-t5/t5-small\"\n",
    "print(f\"Model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load tables.json\n",
    "tables_file = glob.glob(f\"{SPIDER_PATH}/**/tables.json\", recursive=True)\n",
    "if not tables_file:\n",
    "    tables_file = [f\"{SPIDER_PATH}/tables.json\"]\n",
    "\n",
    "with open(tables_file[0]) as f:\n",
    "    tables_data = json.load(f)\n",
    "\n",
    "SCHEMA_LOOKUP = {db[\"db_id\"]: db for db in tables_data}\n",
    "print(f\"Loaded {len(SCHEMA_LOOKUP)} schemas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Load train/dev\n",
    "train_files = [f for f in glob.glob(f\"{SPIDER_PATH}/**/train*.json\", recursive=True) if \"table\" not in f]\n",
    "dev_files = [f for f in glob.glob(f\"{SPIDER_PATH}/**/dev*.json\", recursive=True) if \"table\" not in f]\n",
    "\n",
    "with open(train_files[0]) as f:\n",
    "    train_data = json.load(f)\n",
    "with open(dev_files[0]) as f:\n",
    "    dev_data = json.load(f)\n",
    "\n",
    "print(f\"Train: {len(train_data)} | Dev: {len(dev_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: SIMPLE Schema - just table names and columns\n",
    "def get_schema(db_id):\n",
    "    \"\"\"Simple schema: table1.col1,col2 | table2.col3,col4\"\"\"\n",
    "    if db_id not in SCHEMA_LOOKUP:\n",
    "        return db_id\n",
    "    \n",
    "    db = SCHEMA_LOOKUP[db_id]\n",
    "    tables = db.get(\"table_names\", [])\n",
    "    columns = db.get(\"column_names\", [])\n",
    "    \n",
    "    if not tables or not columns:\n",
    "        return db_id\n",
    "    \n",
    "    table_cols = defaultdict(list)\n",
    "    for col in columns:\n",
    "        if isinstance(col, list) and len(col) >= 2:\n",
    "            tidx, cname = col[0], col[1]\n",
    "            if 0 <= tidx < len(tables):\n",
    "                table_cols[tables[tidx]].append(cname)\n",
    "    \n",
    "    if not table_cols:\n",
    "        return db_id\n",
    "    \n",
    "    # Compact format: table.col1,col2,col3\n",
    "    parts = []\n",
    "    for t, cols in table_cols.items():\n",
    "        cols_str = \",\".join(cols[:6])  # Max 6 cols\n",
    "        parts.append(f\"{t}.{cols_str}\")\n",
    "    \n",
    "    schema = \" | \".join(parts[:5])  # Max 5 tables\n",
    "    return schema[:250]  # Hard limit\n",
    "\n",
    "# DEBUG: Check schemas\n",
    "print(\"Schema samples:\")\n",
    "for i in range(3):\n",
    "    db_id = train_data[i][\"db_id\"]\n",
    "    schema = get_schema(db_id)\n",
    "    print(f\"  {db_id}: {schema[:70]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Process data with VALIDATION\n",
    "from datasets import Dataset\n",
    "\n",
    "def normalize_sql(sql):\n",
    "    return \" \".join(str(sql).lower().split()).rstrip(\";\")\n",
    "\n",
    "def process(data_list):\n",
    "    result = []\n",
    "    bad = 0\n",
    "    \n",
    "    for item in data_list:\n",
    "        q = str(item.get(\"question\", \"\")).strip()\n",
    "        sql = str(item.get(\"query\", \"\")).strip()\n",
    "        db_id = item.get(\"db_id\", \"\")\n",
    "        schema = get_schema(db_id)\n",
    "        \n",
    "        # VALIDATE\n",
    "        if not q or not sql or len(q) < 5 or len(sql) < 5:\n",
    "            bad += 1\n",
    "            continue\n",
    "        \n",
    "        inp = f\"translate to SQL: {q} schema: {schema}\"\n",
    "        tgt = normalize_sql(sql)\n",
    "        \n",
    "        # Length check\n",
    "        if len(inp) > 600 or len(tgt) > 300:\n",
    "            bad += 1\n",
    "            continue\n",
    "        \n",
    "        result.append({\"input_text\": inp, \"target_text\": tgt})\n",
    "    \n",
    "    print(f\"  Valid: {len(result)}, Skipped: {bad}\")\n",
    "    return result\n",
    "\n",
    "print(\"Processing train...\")\n",
    "train_proc = process(train_data)\n",
    "print(\"Processing dev...\")\n",
    "dev_proc = process(dev_data)\n",
    "\n",
    "train_ds = Dataset.from_list(train_proc)\n",
    "dev_ds = Dataset.from_list(dev_proc)\n",
    "\n",
    "# Show samples\n",
    "print(\"\\n--- Samples ---\")\n",
    "for i in range(2):\n",
    "    print(f\"IN: {train_ds[i]['input_text'][:80]}...\")\n",
    "    print(f\"OUT: {train_ds[i]['target_text']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Tokenize\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "def tokenize(batch):\n",
    "    inputs = tokenizer(batch[\"input_text\"], max_length=384, truncation=True)\n",
    "    targets = tokenizer(text_target=batch[\"target_text\"], max_length=128, truncation=True)\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "train_tok = train_ds.map(tokenize, batched=True, remove_columns=train_ds.column_names)\n",
    "dev_tok = dev_ds.map(tokenize, batched=True, remove_columns=dev_ds.column_names)\n",
    "\n",
    "print(f\"Tokenized: {len(train_tok)} train, {len(dev_tok)} dev\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Model\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "model.gradient_checkpointing_enable()\n",
    "print(f\"Loaded {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Training Config - FAST but STABLE\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "\n",
    "collator = DataCollatorForSeq2Seq(tokenizer, model=model, label_pad_token_id=-100)\n",
    "\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./t2sql_v5\",\n",
    "    \n",
    "    num_train_epochs=20,\n",
    "    learning_rate=1e-4,           # Fast LR\n",
    "    warmup_ratio=0.06,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=1.0,            # Gradient clipping\n",
    "    \n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    \n",
    "    fp16=False,                   # OFF for stability\n",
    "    gradient_checkpointing=True,\n",
    "    \n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=128,\n",
    "    generation_num_beams=4,\n",
    "    \n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=0,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(f\"Config: LR={args.learning_rate}, Epochs={args.num_train_epochs}, FP16={args.fp16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Metrics\n",
    "VOCAB_SIZE = len(tokenizer)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    preds = np.clip(preds, 0, VOCAB_SIZE - 1)\n",
    "    \n",
    "    pred_str = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    labels = np.clip(labels, 0, VOCAB_SIZE - 1)\n",
    "    label_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    exact = sum(p.strip() == l.strip() for p, l in zip(pred_str, label_str))\n",
    "    norm = sum(normalize_sql(p) == normalize_sql(l) for p, l in zip(pred_str, label_str))\n",
    "    n = len(pred_str)\n",
    "    \n",
    "    return {\"exact_match\": exact/n, \"normalized_match\": norm/n}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tok,\n",
    "    eval_dataset=dev_tok,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(\"Trainer ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: SANITY CHECK - if loss > 15, something is wrong\n",
    "print(\"Checking initial loss...\")\n",
    "\n",
    "batch = collator([train_tok[i] for i in range(4)])\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(**{k: v.to(model.device) for k, v in batch.items()})\n",
    "    loss = out.loss.item()\n",
    "\n",
    "print(f\"Initial loss: {loss:.2f}\")\n",
    "\n",
    "if loss > 15:\n",
    "    print(\"\\n⚠️ WARNING: Loss too high! Checking data...\")\n",
    "    for i in range(2):\n",
    "        print(f\"Input: {tokenizer.decode(batch['input_ids'][i][:30])}...\")\n",
    "        lbls = [t for t in batch['labels'][i].tolist() if t != -100]\n",
    "        print(f\"Label: {tokenizer.decode(lbls[:20])}...\")\n",
    "else:\n",
    "    print(\"✓ Loss normal. Ready to train.\")\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: TRAIN\n",
    "print(\"=\" * 50)\n",
    "print(\"TRAINING V5 SPIDER\")\n",
    "print(f\"LR: 1e-4 | Epochs: 20 | ~2-3 hours\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "result = trainer.train()\n",
    "\n",
    "print(f\"\\nDone! Loss: {result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Evaluate\n",
    "ev = trainer.evaluate()\n",
    "print(f\"Eval Loss: {ev['eval_loss']:.4f}\")\n",
    "print(f\"Exact Match: {ev['eval_exact_match']*100:.1f}%\")\n",
    "print(f\"Normalized: {ev['eval_normalized_match']*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Save & Zip\n",
    "import shutil\n",
    "\n",
    "trainer.save_model(\"./t2sql_v5_final\")\n",
    "tokenizer.save_pretrained(\"./t2sql_v5_final\")\n",
    "\n",
    "report = {\n",
    "    \"version\": \"v5_spider\",\n",
    "    \"train_loss\": result.training_loss,\n",
    "    \"eval_loss\": ev['eval_loss'],\n",
    "    \"exact_match\": ev['eval_exact_match']*100,\n",
    "    \"normalized_match\": ev['eval_normalized_match']*100,\n",
    "}\n",
    "json.dump(report, open(\"report_v5.json\", \"w\"), indent=2)\n",
    "\n",
    "shutil.make_archive(\"t2sql_v5_spider\", \"zip\", \".\", \"t2sql_v5_final\")\n",
    "print(\"Saved: t2sql_v5_spider.zip\")\n",
    "print(json.dumps(report, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
