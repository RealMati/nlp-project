{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-to-SQL V4 - FIXED Schema Handling\n",
    "\n",
    "**Root Cause Analysis:**\n",
    "- V2/V3 failed because schema wasn't being extracted properly\n",
    "- Model only saw `DATABASE db_name` instead of actual tables/columns\n",
    "- Without schema context, model can't learn table/column names\n",
    "\n",
    "**V4 Fixes:**\n",
    "1. Diagnose dataset structure first\n",
    "2. Robust schema extraction with fallbacks\n",
    "3. Schema included directly in Spider examples\n",
    "4. Fixed regex escaping\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install\n",
    "!pip install -q transformers>=4.35.0 datasets>=2.14.0 accelerate>=0.24.0\n",
    "!pip install -q torch sentencepiece pandas numpy tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TEXT-TO-SQL V4 - FIXED SCHEMA\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    GPU_NAME = torch.cuda.get_device_name(0)\n",
    "    GPU_MEM = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    print(f\"GPU: {GPU_NAME} ({GPU_MEM:.1f} GB)\")\n",
    "    MODEL_NAME = \"google-t5/t5-base\"\n",
    "else:\n",
    "    MODEL_NAME = \"google-t5/t5-small\"\n",
    "    print(\"WARNING: No GPU!\")\n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load Dataset & DIAGNOSE STRUCTURE\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Loading Spider dataset...\")\n",
    "dataset = load_dataset(\"xlangai/spider\")\n",
    "\n",
    "print(f\"\\nTrain: {len(dataset['train'])} | Validation: {len(dataset['validation'])}\")\n",
    "\n",
    "# CRITICAL: Show what's actually in the dataset\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATASET STRUCTURE DIAGNOSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "sample = dataset['train'][0]\n",
    "print(f\"\\nAvailable fields: {list(sample.keys())}\")\n",
    "\n",
    "print(\"\\n--- Sample Values ---\")\n",
    "for key in sample.keys():\n",
    "    val = sample[key]\n",
    "    if isinstance(val, str):\n",
    "        print(f\"{key}: {val[:100]}...\" if len(val) > 100 else f\"{key}: {val}\")\n",
    "    elif isinstance(val, list) and len(val) > 0:\n",
    "        print(f\"{key}: {type(val).__name__}[{len(val)}] = {val[:3]}...\")\n",
    "    else:\n",
    "        print(f\"{key}: {val}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: ROBUST Schema Extraction\n",
    "def extract_schema(example):\n",
    "    \"\"\"\n",
    "    Extract schema from Spider dataset with multiple fallback strategies.\n",
    "    \"\"\"\n",
    "    schema_parts = []\n",
    "    \n",
    "    # Strategy 1: Use db_table_names + db_column_names (if available)\n",
    "    table_names = example.get('db_table_names', [])\n",
    "    column_names = example.get('db_column_names', [])\n",
    "    column_types = example.get('db_column_types', [])\n",
    "    \n",
    "    if table_names and column_names:\n",
    "        table_cols = defaultdict(list)\n",
    "        \n",
    "        for idx, col_info in enumerate(column_names):\n",
    "            # Handle different formats: [table_idx, col_name] or {'table_id': x, 'column_name': y}\n",
    "            if isinstance(col_info, (list, tuple)) and len(col_info) >= 2:\n",
    "                table_idx, col_name = col_info[0], col_info[1]\n",
    "            elif isinstance(col_info, dict):\n",
    "                table_idx = col_info.get('table_id', -1)\n",
    "                col_name = col_info.get('column_name', '')\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            if table_idx == -1 or table_idx >= len(table_names):\n",
    "                continue\n",
    "            \n",
    "            table_name = str(table_names[table_idx]).lower()\n",
    "            col_str = str(col_name).lower()\n",
    "            table_cols[table_name].append(col_str)\n",
    "        \n",
    "        if table_cols:\n",
    "            for tbl, cols in table_cols.items():\n",
    "                schema_parts.append(f\"{tbl}({', '.join(cols)})\")\n",
    "    \n",
    "    # Strategy 2: Use 'schema' field directly if present\n",
    "    if not schema_parts and 'schema' in example:\n",
    "        schema_str = str(example['schema'])\n",
    "        if len(schema_str) > 10:  # Non-trivial schema\n",
    "            return schema_str[:500]  # Truncate if too long\n",
    "    \n",
    "    # Strategy 3: Extract from query (table names from FROM/JOIN clauses)\n",
    "    if not schema_parts:\n",
    "        query = str(example.get('query', '')).lower()\n",
    "        # Extract tables from FROM and JOIN\n",
    "        from_match = re.findall(r'from\\s+(\\w+)', query)\n",
    "        join_match = re.findall(r'join\\s+(\\w+)', query)\n",
    "        tables = set(from_match + join_match)\n",
    "        if tables:\n",
    "            schema_parts = [f\"{t}(*)\" for t in tables]\n",
    "    \n",
    "    # Strategy 4: Just use db_id\n",
    "    if not schema_parts:\n",
    "        db_id = example.get('db_id', 'database')\n",
    "        return f\"database: {db_id}\"\n",
    "    \n",
    "    return \" | \".join(schema_parts)\n",
    "\n",
    "# Test on first few examples\n",
    "print(\"Testing schema extraction:\")\n",
    "for i in range(3):\n",
    "    ex = dataset['train'][i]\n",
    "    schema = extract_schema(ex)\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"  Question: {ex['question'][:60]}...\")\n",
    "    print(f\"  Schema: {schema[:80]}...\" if len(schema) > 80 else f\"  Schema: {schema}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: SQL Normalization (FIXED - no regex issues)\n",
    "def normalize_sql(sql):\n",
    "    \"\"\"\n",
    "    Normalize SQL for comparison. Fixed regex escaping.\n",
    "    \"\"\"\n",
    "    if not sql:\n",
    "        return \"\"\n",
    "    \n",
    "    sql = str(sql).strip().lower()\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    sql = ' '.join(sql.split())\n",
    "    \n",
    "    # Normalize operators (simple string replacement, not regex)\n",
    "    for op in ['>=', '<=', '!=', '<>', '=', '>', '<']:\n",
    "        sql = sql.replace(op, f' {op} ')\n",
    "    \n",
    "    # Normalize commas\n",
    "    sql = sql.replace(',', ', ')\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    sql = ' '.join(sql.split())\n",
    "    \n",
    "    # Remove trailing semicolon\n",
    "    sql = sql.rstrip(';').strip()\n",
    "    \n",
    "    return sql\n",
    "\n",
    "# Test\n",
    "test_cases = [\n",
    "    \"SELECT COUNT(*)  FROM  students   WHERE gpa>3.5;\",\n",
    "    \"select * from users where age >= 18\",\n",
    "    \"SELECT name,age FROM people\"\n",
    "]\n",
    "\n",
    "print(\"SQL Normalization Test:\")\n",
    "for sql in test_cases:\n",
    "    print(f\"  {sql}\")\n",
    "    print(f\"  → {normalize_sql(sql)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Preprocessing with VERIFIED Schema\n",
    "def preprocess_v4(example):\n",
    "    question = str(example.get('question', '')).strip()\n",
    "    sql = str(example.get('query', '')).strip()\n",
    "    schema = extract_schema(example)\n",
    "    \n",
    "    # Format: explicit instruction with schema\n",
    "    input_text = f\"translate to SQL: {question} | schema: {schema}\"\n",
    "    target_text = normalize_sql(sql)\n",
    "    \n",
    "    return {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text\n",
    "    }\n",
    "\n",
    "print(\"Preprocessing...\")\n",
    "processed = dataset.map(preprocess_v4, num_proc=4)\n",
    "\n",
    "# Verify the preprocessing worked\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PREPROCESSING VERIFICATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i in range(3):\n",
    "    ex = processed['train'][i]\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"Input: {ex['input_text'][:150]}...\")\n",
    "    print(f\"Target: {ex['target_text']}\")\n",
    "\n",
    "# Check schema coverage\n",
    "has_schema = sum(1 for ex in processed['train'] if 'schema:' in ex['input_text'] and len(ex['input_text'].split('schema:')[1]) > 20)\n",
    "print(f\"\\nExamples with meaningful schema: {has_schema}/{len(processed['train'])} ({100*has_schema/len(processed['train']):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Tokenization\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "print(f\"Loading tokenizer: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "MAX_INPUT = 512\n",
    "MAX_TARGET = 256\n",
    "\n",
    "def tokenize(examples):\n",
    "    inputs = tokenizer(\n",
    "        examples[\"input_text\"],\n",
    "        max_length=MAX_INPUT,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    targets = tokenizer(\n",
    "        text_target=examples[\"target_text\"],\n",
    "        max_length=MAX_TARGET,\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )\n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
    "    return inputs\n",
    "\n",
    "print(\"Tokenizing...\")\n",
    "tokenized = processed.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=processed['train'].column_names\n",
    ")\n",
    "\n",
    "print(f\"Done. Columns: {tokenized['train'].column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Load Model\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "print(f\"Parameters: {model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Training Configuration\n",
    "from transformers import (\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    padding=True\n",
    ")\n",
    "\n",
    "# V4 Settings - balanced approach\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./t2sql_v4\",\n",
    "    \n",
    "    num_train_epochs=25,           # More epochs for convergence\n",
    "    learning_rate=2e-4,            # Middle ground\n",
    "    warmup_ratio=0.06,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    \n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=16,\n",
    "    gradient_accumulation_steps=4,  # Effective batch 32\n",
    "    \n",
    "    weight_decay=0.01,\n",
    "    \n",
    "    fp16=torch.cuda.is_available(),\n",
    "    gradient_checkpointing=True,\n",
    "    label_smoothing_factor=0.1,\n",
    "    \n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    \n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=MAX_TARGET,\n",
    "    generation_num_beams=4,\n",
    "    \n",
    "    logging_steps=50,\n",
    "    report_to=\"none\",\n",
    "    dataloader_num_workers=2,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "print(\"V4 Configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  LR: {training_args.learning_rate}\")\n",
    "print(f\"  Effective batch: 32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Metrics\n",
    "VOCAB_SIZE = len(tokenizer)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    predictions = np.clip(predictions, 0, VOCAB_SIZE - 1)\n",
    "    \n",
    "    try:\n",
    "        pred_texts = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    except:\n",
    "        return {\"exact_match\": 0.0, \"normalized_match\": 0.0}\n",
    "    \n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    labels = np.clip(labels, 0, VOCAB_SIZE - 1)\n",
    "    \n",
    "    try:\n",
    "        label_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    except:\n",
    "        return {\"exact_match\": 0.0, \"normalized_match\": 0.0}\n",
    "    \n",
    "    exact = 0\n",
    "    normalized = 0\n",
    "    total = len(pred_texts)\n",
    "    \n",
    "    for pred, label in zip(pred_texts, label_texts):\n",
    "        if pred.strip() == label.strip():\n",
    "            exact += 1\n",
    "        if normalize_sql(pred) == normalize_sql(label):\n",
    "            normalized += 1\n",
    "    \n",
    "    return {\n",
    "        \"exact_match\": exact / total if total > 0 else 0.0,\n",
    "        \"normalized_match\": normalized / total if total > 0 else 0.0\n",
    "    }\n",
    "\n",
    "print(\"Metrics ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Initialize Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(f\"Trainer ready. Train: {len(tokenized['train'])} | Eval: {len(tokenized['validation'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Verification\n",
    "print(\"Final verification...\")\n",
    "\n",
    "test_batch = [tokenized['train'][i] for i in range(2)]\n",
    "collated = data_collator(test_batch)\n",
    "print(\"✓ Collator OK\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(**{k: v.to(model.device) for k, v in collated.items()})\n",
    "print(f\"✓ Forward OK (loss: {out.loss.item():.2f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"V4 READY TO TRAIN\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: TRAIN\n",
    "print(\"=\" * 60)\n",
    "print(\"STARTING V4 TRAINING\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Epochs: 25 | LR: 2e-4\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "result = trainer.train()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"V4 TRAINING COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Train loss: {result.training_loss:.4f}\")\n",
    "print(f\"Time: {result.metrics['train_runtime']/3600:.2f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Evaluate\n",
    "print(\"Evaluating...\\n\")\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"V4 RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Eval Loss: {eval_results['eval_loss']:.4f}\")\n",
    "print(f\"Exact Match: {eval_results['eval_exact_match']*100:.2f}%\")\n",
    "print(f\"Normalized Match: {eval_results['eval_normalized_match']*100:.2f}%\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "nm = eval_results['eval_normalized_match'] * 100\n",
    "if nm >= 40:\n",
    "    grade = \"EXCELLENT\"\n",
    "elif nm >= 30:\n",
    "    grade = \"GOOD\"\n",
    "elif nm >= 20:\n",
    "    grade = \"ACCEPTABLE\"\n",
    "else:\n",
    "    grade = \"NEEDS WORK\"\n",
    "print(f\"\\nGrade: {grade}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Save Model\n",
    "OUTPUT = \"./t2sql_final_v4\"\n",
    "\n",
    "print(f\"Saving to {OUTPUT}...\")\n",
    "trainer.save_model(OUTPUT)\n",
    "tokenizer.save_pretrained(OUTPUT)\n",
    "\n",
    "report = {\n",
    "    \"version\": \"v4_fixed_schema\",\n",
    "    \"model\": MODEL_NAME,\n",
    "    \"epochs\": 25,\n",
    "    \"learning_rate\": \"2e-4\",\n",
    "    \"train_loss\": result.training_loss,\n",
    "    \"eval_loss\": eval_results['eval_loss'],\n",
    "    \"exact_match_pct\": eval_results['eval_exact_match'] * 100,\n",
    "    \"normalized_match_pct\": eval_results['eval_normalized_match'] * 100,\n",
    "    \"training_hours\": result.metrics['train_runtime'] / 3600\n",
    "}\n",
    "\n",
    "with open(\"report_v4.json\", \"w\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "print(\"Saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16: Test Predictions\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"Testing V4 model...\\n\")\n",
    "\n",
    "gen = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=OUTPUT,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "def predict(q, schema):\n",
    "    inp = f\"translate to SQL: {q} | schema: {schema}\"\n",
    "    out = gen(inp, max_length=256, num_beams=4)\n",
    "    return out[0]['generated_text']\n",
    "\n",
    "tests = [\n",
    "    (\"How many students are there?\", \"students(id, name, age, gpa)\"),\n",
    "    (\"Find students with GPA above 3.5\", \"students(id, name, gpa)\"),\n",
    "    (\"List departments with average salary\", \"employees(id, name, dept, salary)\"),\n",
    "    (\"Count employees per department\", \"employees(id, name, department)\"),\n",
    "]\n",
    "\n",
    "for q, s in tests:\n",
    "    sql = predict(q, s)\n",
    "    print(f\"Q: {q}\")\n",
    "    print(f\"Schema: {s}\")\n",
    "    print(f\"SQL: {sql}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Zip Model\n",
    "import shutil\n",
    "\n",
    "print(\"Zipping model...\")\n",
    "shutil.make_archive(\"t2sql_v4_model\", \"zip\", \".\", \"t2sql_final_v4\")\n",
    "print(\"Created: t2sql_v4_model.zip\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"V4 FINAL REPORT\")\n",
    "print(\"=\" * 60)\n",
    "print(json.dumps(report, indent=2))\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nDownload: t2sql_v4_model.zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
